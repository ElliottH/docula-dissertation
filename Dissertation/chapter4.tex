% This is where Assessors will be looking for signs of success and for evidence
% of thorough and systematic testing. Sample output, tables of timings and
% photographs of workstation screens, oscilloscope traces or circuit boards may
% be included.

% As with code, voluminous examples of sample output are usually best left to
% appendices or omitted altogether.

% There are some obvious questions which this chapter will address. How many of
% the original goals were achieved? Were they proved to have been achieved? Did
% the program/hardware/theory really work?

% Assessors are well aware that large programs will very likely include some
% residual bugs. It should always be possible to demonstrate that a program
% works in simple cases and it is instructive to demonstrate how close it is to
% working in a really ambitious case.

\chapter{Evaluation}
\emph{Full-size versions of all graphs in this section, and their corresponding
data, can be found in the `Results' chapter of the Appendix.}

\section{Speed Testing}
An important aspect of this project is speed, this is because users will not
want to wait excessive lengths of time for the tool to complete, particularly if
it is expected to be used incrementally, as this is. In order to test this, I
ran the software over several open-source projects and measured their runtime;
as a comparision, I also used doxygen on the same projects. As my project stores
information from files so that future runs are quicker, I ran the tool a second
time over the source trees to examine how much effect this has; doxygen does not
do any such storing, and so only one execution was required per project.

In the interests of ensuring that the measurements were correct, this process
was performed several times for each project, and the results were averaged. In
each case, results were pretty consistent.

Since the software failed on some source files due to problems with my parser,
not all files were successfully parsed; to account for this, I decided to
estimate the total time the software would have taken. When the parser fails,
the line on which it failed can be retrieved, I used this to determine the
fraction of the file that was successfully parsed and estimated the time that
would be taken. This allows for a fairer comparison with doxygen.

\noindent\makebox[\textwidth]{%
  \includegraphics[width=160mm]{Graphs/timings.pdf}
}

Above is a graph comparing the execution times of my project \& doxygen, the
bars for my software are broken down into three parts: the actual runtime,
projected runtime, and the time for a subsequent run.

These results demonstrate that doxygen is generally faster than my software on
the first run, even without accounting for the failed files; however, in all but
one test, my software out-performs doxygen on subsequent runs. These results are
fairly in-line with my expectations: doxygen is written in C++ and therefore has
less of an overhead compared to my solution, not to mention that its developers
have had much longer to tune the performance of their software. I had also
expected that my use of a database would allow for a dramatic increase in speed,
and I am pleased to see that this was the case.

  \subsection{Profiling}
  In addition to running the above tests, I also opted to profile the code to
  try and work out what sections were causing the high execution times. I had
  expected to find that the I/O to the database on disk was the cause of it, but
  the results of profiling demonstrated that the parser was in fact the slowest
  component; in particular, the sheer number of classes that Treetop
  instantiated caused calls to \lstinline|Class#new| and
  \lstinline|Kernel#extend| to take a significant proportion ($\sim$8\%) of the
  total execution time!

  Upon further investigation I discovered why Treetop was instantiating so
  many classes, it would appear that any match to part of a defined rule
  creates a \lstinline|SyntaxNode|. The rules for defining comments illustrate
  this problem well: part of its definition contains ``\lstinline|( !EOL .)*|''
  which matches any character other than a newline, causing a
  \lstinline|SyntaxNode| to be created for \emph{every character in the
  comment}. Fortunately, this means that the parser's performance could be
  improved with better regular expressions.

\section{Correctness Testing}

In order to assess whether my software was successfully displaying the
documentation that it had extracted from the source code it had been run on, I
needed to check the generated HTML documents; I opted to do this by a
semi-automated approach, since doing it entirely manually would have taken an
extremely long time.

To expedite this process, I wrote some code that counted the number of
\lstinline|/**|
in a file and compared it to the number of documentation strings mentioned in
the HTML output. For any file that these two numbers did not match, I
investigated what was occurring.

I noticed quickly that there was a particular type that was being consistently
ignored, which were the ones used by doxygen to denote the beginning \& end of a
group of definitions that are linked; these comments are of the form
\lstinline|/** @{ */| and \lstinline|/** @} */| respectively. These comments
do not generally contain any actual documentation, and in the cases they did
it was successfully extracted, so I accounted for the occurrence of these in
code I was using to verify the documentation. Once I removed these false
positives, the results looked much more positive, as can be seen in the graph
below.

\noindent\makebox[\textwidth]{%
  \includegraphics[width=160mm]{Graphs/correct.pdf}
}

Here, the graph is broken down into three parts, the documentation that was
successfully displayed, those that contained grouping information \& the total
number of documentation strings in a file. The graph shows that the majority
of the documentation was successfully displayed.

Having analysed the files that did not have the correct amount of information
displayed in the output, I noticed that the failures fell into two categories:
\begin{itemize}
  \item I am failing to display the documentation that is contained in the
    bodies of structs, and similar data structures, which is causing the large
    majority of the discrepancies in the graph.
    \begin{itemize}
      \item In some situations, my code appears to mistake a typedef
      containing one of these data structures for a normal typedef, and
      therefore doesn't print them correctly at all.
    \end{itemize}
  \item Documentation comments placed before items that I do not consider for
    outputting at all, such as \lstinline|#ifdef| statements. doxygen's
    solution to this is to associate the documentation with the related
    \lstinline|#define|.
\end{itemize}

Aside from these two situations, it appears that if a file is successfully
parsed, all of its documentation will make it into the HTML documents. Both of
the above failures are due to minor oversights in my code, rather than glaring
problems with implementation decisions.

\section{Functionality Testing}
