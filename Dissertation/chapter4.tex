% This is where Assessors will be looking for signs of success and for evidence
% of thorough and systematic testing. Sample output, tables of timings and
% photographs of workstation screens, oscilloscope traces or circuit boards may
% be included.

% As with code, voluminous examples of sample output are usually best left to
% appendices or omitted altogether.

% There are some obvious questions which this chapter will address. How many of
% the original goals were achieved? Were they proved to have been achieved? Did
% the program/hardware/theory really work?

% Assessors are well aware that large programs will very likely include some
% residual bugs. It should always be possible to demonstrate that a program
% works in simple cases and it is instructive to demonstrate how close it is to
% working in a really ambitious case.

\chapter{Evaluation}
\emph{Full-size versions of all graphs in this section, and their corresponding
data, can be found in the `Results' chapter of the Appendix.}

For the tests below, I used the source code for several open-source projects
that are available online; some of these were found on doxygen's website as
examples of projects that contain doxygen-style documentation, others are
projects that I have encountered or worked with in the past that I knew to
contain documentation.

I will be using these source bases to evaluate the speed with which my software
executes, whether my software correctly outputs the documentation obtained and
how people find my software to use.

\section{Speed Testing}
An important aspect of this project is speed. This is because users will not
want to wait excessive lengths of time for the tool to complete, particularly if
it is expected to be used incrementally, as this is. In order to test this, I
ran the software over several open-source projects and measured their runtime;
as a comparision, I also used doxygen on the same projects. As my project caches
from files in the database so that future runs are quicker, I ran the tool a
second time over the source trees to examine how much effect this has; doxygen
does not do any such storing, and so only one execution was required per
project.

In the interests of ensuring that the measurements were correct, this process
was performed several times for each project, and the results were averaged. In
each case, results were pretty consistent.

Since the software failed on some source files due to problems with my parser,
not all files were successfully parsed; to account for this, I decided to
estimate the total time the software would have taken. When the parser fails,
the line on which it failed can be retrieved, I used this to determine the
fraction of the file that was successfully parsed and estimated the time that
would be taken. This allows for a fairer comparison with doxygen.

\noindent\makebox[\textwidth]{%
  \includegraphics[width=160mm]{Graphs/timings.pdf}
}

Above is a graph comparing the execution times of my project and doxygen, the
bars for my software are broken down into three parts: the actual runtime,
projected runtime, and the time for a subsequent run.

These results demonstrate that doxygen is generally faster than my software on
the first run, even without accounting for the failed files; however, in all but
one test, my software out-performs doxygen on subsequent runs. These results are
fairly in-line with my expectations: doxygen is written in C++ and therefore has
less of an overhead compared to my solution, not to mention that its developers
have had much longer to tune the performance of their software. I had also
expected that my use of a database would allow for a dramatic increase in speed
on subsequent runs, and I am pleased to see that this was the case.

  \subsection{Profiling}
  In addition to running the above tests, I also opted to profile the code to
  try and work out what sections were causing the high execution times. I had
  expected to find that the I/O to the database on disk was the cause, but
  the results of profiling demonstrated that the parser was in fact the slowest
  component; in particular, the sheer number of classes that Treetop
  instantiated caused calls to \lstinline|Class#new| and
  \lstinline|Kernel#extend| to take a significant proportion ($\sim$10\%) of the
  total execution time!

  Upon further investigation I discovered why Treetop was instantiating so
  many classes, it would appear that any match to part of a defined rule
  creates a \lstinline|SyntaxNode|. The rules for defining comments illustrate
  this problem well: part of its definition contains ``\lstinline|( !EOL .)*|''
  which matches any character other than a newline, causing a
  \lstinline|SyntaxNode| to be created for \emph{every character in the
  comment}. Fortunately, this means that the parser's performance could be
  improved with better regular expressions.

  I used the in-built `profile' library to perform my profiling, which just
  needed to be included like any other library using `require'; profile adds
  hooks to the function calls in the running code, which it uses to record
  how long they take to execute and how many times they are called. When the
  code finishes executing, profile outputs a table of information like so:

\begingroup
    \fontsize{8pt}{12pt}\selectfont
    \begin{verbatim}
 %   cumulative   self              self     total
time   seconds   seconds    calls  ms/call  ms/call  name
12.49    25.41     25.41  1181040     0.02     0.03  Treetop::Runtime::CompiledParser#has_terminal?
10.45    46.67     21.26  1457515     0.01     0.03  Treetop::Runtime::CompiledParser#instantiate_node
 8.51    63.98     17.31   842122     0.02     0.03  Treetop::Runtime::CompiledParser#terminal_parse_failure
 6.79    77.80     13.82   119589     0.12     1.73  CSimple#_nt_function_body_item
 6.33    90.67     12.87  1478676     0.01     0.01  Class#new
 5.72   102.30     11.63   140154     0.08     0.60  CSimple#_nt_comment_contents
 4.44   111.34      9.04   217330     0.04     0.10  CSimple#_nt_end_comment
 3.75   118.96      7.62   470575     0.02     0.02  Kernel#extend
 3.51   126.11      7.15    78088     0.09     0.74  CSimple#_nt_docstring_contents
 2.82   131.85      5.74    66223     0.09     1.49  CSimple#_nt_directive_line
# [rest of output ommitted]
  \end{verbatim}
\endgroup

  I profiled my software on libavutil, which is part of ffmpeg; I selected
  this because it was fairly small, and profile causes the code to execute
  more slowly, due to the overhead of all the hooks used.

\section{Correctness Testing}

In order to assess whether my software was successfully displaying the
documentation that it had extracted from the source code it had been run on, I
needed to check the generated HTML documents; I opted to automate this process,
as a manual scan would have taken a long time - particularly as the process
would have had to have been repeated for subsequent versions of the program.


To expedite this process, I wrote some code that counted the number of
\lstinline|/**|
in a file and compared it to the number of documentation strings mentioned in
the HTML output. Files which showed a mismatch were investigated manually.

I noticed quickly that there was a particular type that was being consistently
ignored, which were the ones used by doxygen to denote the beginning and end of
a group of definitions that are linked; these comments are of the form
\lstinline|/** @{ */| and \lstinline|/** @} */| respectively. These comments
do not generally contain any actual documentation, and in the cases they did
it was successfully extracted, so I accounted for the occurrence of these in
code I was using to verify the documentation. Once I removed these false
positives, the results looked much more positive, as can be seen in the graph
below.

\noindent\makebox[\textwidth]{%
  \includegraphics[width=160mm]{Graphs/correct.pdf}\\
}
\begin{center}
  \emph{Graph showing the results of running the code over libavutil}
\end{center}

Here, the graph is broken down into three parts, the documentation that was
successfully displayed, those that contained grouping information and the total
number of documentation strings in a file. The graph shows that the majority
of the documentation was successfully displayed.

Having analysed the files that did not have the correct amount of information
displayed in the output, I noticed that the failures fell into two categories:
\begin{itemize}
  \item I am failing to display the documentation that is contained in the
    bodies of structs, and similar data structures, which is causing the large
    majority of the discrepancies in the graph.
    \begin{itemize}
      \item In some situations, my code appears to mistake a typedef
      containing one of these data structures for a normal typedef, and
      therefore doesn't print them correctly at all.
    \end{itemize}
  \item Documentation comments placed before items that I do not consider for
    outputting at all, such as \lstinline|#ifdef| statements. doxygen's
    solution to this is to associate the documentation with the related
    \lstinline|#define|.
\end{itemize}

Aside from these two situations, it appears that if a file is successfully
parsed, all of its documentation will make it into the HTML documents. Both of
the above failures are due to minor oversights in my code, rather than glaring
problems with implementation decisions.

\section{Functionality Testing}
This section of the testing was to assess whether the software created
produces useful information for the user; this was done using a questionnaire,
which asked users to complete some tasks using the software, some source code
and the generated documentation.

I deliberately chose to use a specific set of source files for the purposes of
the questionnaire so that the users' experiences are comparable with one
another, and so that I could ask well-defined questions in the questionnaire.
The questionnaire tasked the users with finding out information about a
function, adding their own documentation to one file and asking them about how
they found the process.

In the lookup task, two-thirds of people successfully found the information
describing what was returned by a particular function in the event of an
error. Interestingly, the same two users that were unable to find the function
also complained of problems with the files listing not being sorted; this
appears to be due to the \lstinline|Dir.foreach| method, since different Ruby
implementations appear to return this data sorted in different ways (or not at
all). I never encountered this problem as I did most of my development on an OS
X machine, and the ordering of files returned is consistently alphabetical.

83\% of the users successfully had the information they added to a file
successfully appear in the generated documentation; of those users, only 40\%
said that the documentation appeared as expected, with most commenting that
\lstinline|@param| annotations were not displayed. Unfortunately, this appears
to be due to an undetermined bug, which cannot be produced on my end, and was
not experienced by all users. Further investigation into the cause of this
would be needed if development were to continue.

Oddly, one-third of people surveyed did not experience any speed-up on the
second execution of the software, whilst everyone else noticed a significant
difference. Given that my testing has already demonstrated that there is a
consistent speed up, and that the majority of users encountered this also, I
believe that this was due to other operations taking place on the users'
machines whilst the second execution was taking place. Additionally, the
set of files used for testing here was much smaller than the sets used in my
other testing, so the speed-up may have gone unnoticed due to short execution
times on both executions of the software.

The results of these questionnaires show that, whilst people were generally able
to use the software and the output documentation, there were some usability
issues involved. The majority of the people surveyed commented that a search
would have been extremely useful; I would very much have liked to implement
this, without a server-side component, however, it would have been extremely
difficult to produce a search that worked in the browser. Navigation between
files was also cited as a problem, which could be improved simply by having
links back to the files listing rather than relying on the back and forward
navigation available in the browser.




















